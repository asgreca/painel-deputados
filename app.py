import os
import requests
import streamlit as st
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import asyncio

# üìå T√≠tulo do aplicativo no Streamlit
st.title("üó£Ô∏è An√°lise Pol√≠tica - Discursos dos Deputados Federais")

# ================================================================
# üîπ 1) INICIALIZAR SESSION STATE (API KEY, Modelo, etc.)
# ================================================================
if "openai_api_key" not in st.session_state:
    st.session_state.openai_api_key = None
if "selected_model" not in st.session_state:
    st.session_state.selected_model = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# üîë Entrada para a API Key da OpenAI
if not st.session_state.openai_api_key:
    st.session_state.openai_api_key = st.text_input(
        "Insira sua API Key da OpenAI:",
        type="password",
        placeholder="Cole sua API Key aqui",
    )

if not st.session_state.openai_api_key:
    st.warning("Por favor, insira sua API Key para continuar.")
    st.stop()

# üîπ Lista de modelos dispon√≠veis
model_options = {
    "gpt-4o": "Modelo vers√°til e de alta intelig√™ncia.",
    "gpt-4o-mini": "Modelo menor, r√°pido e acess√≠vel, ideal para tarefas espec√≠ficas.",
}

if not st.session_state.selected_model:
    st.session_state.selected_model = st.selectbox(
        "Escolha o modelo GPT:",
        options=[""] + list(model_options.keys()),
        format_func=lambda x: f"{x} - {model_options.get(x, 'Selecione um modelo')}" if x else "Selecione um modelo",
    )

if not st.session_state.selected_model:
    st.warning("Por favor, escolha um modelo GPT para continuar.")
    st.stop()

# ================================================================
# üìå CONFIGURA√á√ÉO DO ENDPOINT ChromaDB
# ================================================================
CHROMA_API_URL = "https://chroma-production-6065.up.railway.app/api/v1/collections/40535baa-0a68-4862-9e4c-1963f4981795/query"

# ================================================================
# üîπ 2) CRIAR EMBEDDINGS E MODELO LLM
# ================================================================
if "embedder" not in st.session_state:
    try:
        st.session_state.embedder = OpenAIEmbeddings(
            openai_api_key=st.session_state.openai_api_key,
            model="text-embedding-ada-002"
        )
    except Exception as e:
        st.error(f"Erro ao criar embeddings: {str(e)}")
        st.stop()

if "llm" not in st.session_state:
    try:
        st.session_state.llm = ChatOpenAI(
            model=st.session_state.selected_model,
            temperature=0.5,
            openai_api_key=st.session_state.openai_api_key,
        )
    except Exception as e:
        st.error(f"Erro ao inicializar o modelo de linguagem: {str(e)}")
        st.stop()

# ================================================================
# üîé Fun√ß√£o para consultar o ChromaDB (via embeddings)
# ================================================================
async def buscar_contexto(query_text):
    """
    Gera embedding do texto e faz POST no endpoint /query do ChromaDB,
    retornando documentos relevantes.
    """
    try:
        # Gerar embedding usando OpenAI
        st.write("üîç Gerando embedding para a consulta...")
        embedding = await st.session_state.embedder.aembed_query(query_text)

        # Preparar o payload
        payload = {
            "query_embeddings": [embedding],
            "n_results": 5,
            "include": ["documents", "metadatas", "distances"]
        }
        st.write("üì§ Payload enviado ao ChromaDB:", payload)

        # Fazer a requisi√ß√£o
        response = requests.post(
            CHROMA_API_URL,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )

        # Log da resposta
        st.write("üîç Status da resposta:", response.status_code)
        st.write("üì• Conte√∫do da resposta:", response.text)

        if response.status_code == 200:
            data = response.json()
            docs = data.get("documents", [[]])
            if not docs or not docs[0]:
                return "Nenhum contexto relevante foi encontrado."
            return "\n\n".join(doc[:5000] for doc in docs[0])
        else:
            return f"Erro ao buscar contexto: {response.status_code} - {response.text}"

    except Exception as e:
        st.error(f"‚ùå Erro ao se conectar com ChromaDB: {str(e)}")
        return f"Erro ao se conectar com ChromaDB: {str(e)}"

# ================================================================
# üîπ 3) TESTAR CONEX√ÉO A PARTIR DE UMA PERGUNTA DE EXEMPLO
# ================================================================
def testar_conexao_chromadb():
    """
    Faz uma consulta de teste ao ChromaDB para verificar conectividade.
    """
    st.info("Testando conex√£o com ChromaDB usando a query 'teste de conex√£o'...")

    try:
        contexto_teste = asyncio.run(buscar_contexto("teste de conex√£o"))

        if "Erro ao buscar contexto" in contexto_teste or "Erro ao se conectar" in contexto_teste:
            st.error(f"‚ùå Falha ao conectar ou buscar contexto: {contexto_teste}")
        else:
            st.success("‚úÖ Conex√£o OK! Resposta de teste obtida.")
            st.write("**Resposta do ChromaDB:**")
            st.write(contexto_teste)

    except Exception as e:
        st.error(f"‚ùå Erro inesperado ao testar conex√£o com ChromaDB: {str(e)}")

if "test_conexao_feito" not in st.session_state:
    st.session_state.test_conexao_feito = True
    testar_conexao_chromadb()

# ================================================================
# üîπ 4) SETUP DO PROMPT E DA CADEIA LLM
# ================================================================
prompt_text = """
Voc√™ √© um analista pol√≠tico. Sua fun√ß√£o √© analisar os discursos exclusivamente dos deputados federais 
e fornecer respostas detalhadas e objetivas com base no hist√≥rico e no contexto abaixo. N√£o fale frases gen√©ricas e evasivas, sempre indique a origem da ideia, onde estava registrada como em qual comiss√£o, data e n√∫mero da reuni√£o. A resposta a ser dada deve ter sempre pelo menos 3 par√°grafos.

Hist√≥rico:
{history}

Contexto:
{context}

Pergunta:
{question}

Resposta:
"""
prompt = PromptTemplate(
    input_variables=["history", "context", "question"],
    template=prompt_text
)
chain = LLMChain(llm=st.session_state.llm, prompt=prompt)

# ================================================================
# üí¨ 5) EXIBIR HIST√ìRICO & INTERA√á√ÉO NO CHAT
# ================================================================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_input := st.chat_input("Digite sua pergunta:"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # üîç Buscar contexto no ChromaDB
    contexto = asyncio.run(buscar_contexto(user_input))

    try:
        response = asyncio.run(chain.arun(history="", context=contexto, question=user_input))
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.chat_message("assistant").markdown(response)
    except Exception as e:
        st.error(f"Erro ao gerar resposta: {e}")
